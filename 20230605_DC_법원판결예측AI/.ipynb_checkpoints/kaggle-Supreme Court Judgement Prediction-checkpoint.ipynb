{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2046e6d8",
   "metadata": {},
   "source": [
    "[kaggle 코드 참고-Supreme Court Judgement Prediction](https://www.kaggle.com/code/raghavkachroo/supreme-court-judgement-prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49ca3062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55cceca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "422a66c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14ca4309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19e48f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰화에서 필요\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d85b8cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임포트\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report # 평가 지표 확인\n",
    "\n",
    "# XGBoost : Extreme Gradient Boosting의 약자\n",
    "# # 기존 Gradient Tree Boosting 알고리즘에 과적합 방지를 위한 기법이 추가된 지도학습 알고리즘\n",
    "\n",
    "from xgboost import XGBClassifier \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import re\n",
    "import nltk # Natural Language Toolkit 자연어 처리를 위함\n",
    "import spacy\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8ea5793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "open/.DS_Store\n",
      "open/open.zip\n"
     ]
    }
   ],
   "source": [
    "# 파일 경로 파악\n",
    "for dirname, _, filenames in os.walk('open'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ac4f410",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>first_party</th>\n",
       "      <th>second_party</th>\n",
       "      <th>facts</th>\n",
       "      <th>first_party_winner</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRAIN_0000</td>\n",
       "      <td>Phil A. St. Amant</td>\n",
       "      <td>Herman A. Thompson</td>\n",
       "      <td>On June 27, 1962, Phil St. Amant, a candidate ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRAIN_0001</td>\n",
       "      <td>Stephen Duncan</td>\n",
       "      <td>Lawrence Owens</td>\n",
       "      <td>Ramon Nelson was riding his bike when he suffe...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAIN_0002</td>\n",
       "      <td>Billy Joe Magwood</td>\n",
       "      <td>Tony Patterson, Warden, et al.</td>\n",
       "      <td>An Alabama state court convicted Billy Joe Mag...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRAIN_0003</td>\n",
       "      <td>Linkletter</td>\n",
       "      <td>Walker</td>\n",
       "      <td>Victor Linkletter was convicted in state court...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRAIN_0004</td>\n",
       "      <td>William Earl Fikes</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>On April 24, 1953 in Selma, Alabama, an intrud...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ID         first_party                    second_party  \\\n",
       "0  TRAIN_0000   Phil A. St. Amant              Herman A. Thompson   \n",
       "1  TRAIN_0001      Stephen Duncan                  Lawrence Owens   \n",
       "2  TRAIN_0002   Billy Joe Magwood  Tony Patterson, Warden, et al.   \n",
       "3  TRAIN_0003          Linkletter                          Walker   \n",
       "4  TRAIN_0004  William Earl Fikes                         Alabama   \n",
       "\n",
       "                                               facts  first_party_winner  \n",
       "0  On June 27, 1962, Phil St. Amant, a candidate ...                   1  \n",
       "1  Ramon Nelson was riding his bike when he suffe...                   0  \n",
       "2  An Alabama state court convicted Billy Joe Mag...                   1  \n",
       "3  Victor Linkletter was convicted in state court...                   0  \n",
       "4  On April 24, 1953 in Selma, Alabama, an intrud...                   1  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 로드\n",
    "df = pd.read_csv('train.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70ee6f14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2478 entries, 0 to 2477\n",
      "Data columns (total 5 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   ID                  2478 non-null   object\n",
      " 1   first_party         2478 non-null   object\n",
      " 2   second_party        2478 non-null   object\n",
      " 3   facts               2478 non-null   object\n",
      " 4   first_party_winner  2478 non-null   int64 \n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 96.9+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3da69d84",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>first_party</th>\n",
       "      <th>second_party</th>\n",
       "      <th>facts</th>\n",
       "      <th>first_party_winner</th>\n",
       "      <th>facts_replace</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRAIN_0000</td>\n",
       "      <td>Phil A. St. Amant</td>\n",
       "      <td>Herman A. Thompson</td>\n",
       "      <td>On June 27, 1962, Phil St. Amant, a candidate ...</td>\n",
       "      <td>1</td>\n",
       "      <td>On June 27, 1962, Phil St. Amant, a candidate ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRAIN_0001</td>\n",
       "      <td>Stephen Duncan</td>\n",
       "      <td>Lawrence Owens</td>\n",
       "      <td>Ramon Nelson was riding his bike when he suffe...</td>\n",
       "      <td>0</td>\n",
       "      <td>Ramon Nelson was riding his bike when he suffe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAIN_0002</td>\n",
       "      <td>Billy Joe Magwood</td>\n",
       "      <td>Tony Patterson, Warden, et al.</td>\n",
       "      <td>An Alabama state court convicted Billy Joe Mag...</td>\n",
       "      <td>1</td>\n",
       "      <td>An Alabama state court convicted Billy Joe Mag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRAIN_0003</td>\n",
       "      <td>Linkletter</td>\n",
       "      <td>Walker</td>\n",
       "      <td>Victor Linkletter was convicted in state court...</td>\n",
       "      <td>0</td>\n",
       "      <td>Victor Linkletter was convicted in state court...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRAIN_0004</td>\n",
       "      <td>William Earl Fikes</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>On April 24, 1953 in Selma, Alabama, an intrud...</td>\n",
       "      <td>1</td>\n",
       "      <td>On April 24, 1953 in Selma, Alabama, an intrud...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2473</th>\n",
       "      <td>TRAIN_2473</td>\n",
       "      <td>HollyFrontier Cheyenne Refining, LLC, et al.</td>\n",
       "      <td>Renewable Fuels Association, et al.</td>\n",
       "      <td>Congress amended the Clean Air Act through the...</td>\n",
       "      <td>1</td>\n",
       "      <td>Congress amended the Clean Air Act through the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2474</th>\n",
       "      <td>TRAIN_2474</td>\n",
       "      <td>Grupo Mexicano de Desarrollo, S. A.</td>\n",
       "      <td>Alliance Bond Fund, Inc.</td>\n",
       "      <td>Alliance Bond Fund, Inc., an investment fund, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Alliance Bond Fund, Inc., an investment fund, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2475</th>\n",
       "      <td>TRAIN_2475</td>\n",
       "      <td>Peguero</td>\n",
       "      <td>United States</td>\n",
       "      <td>In 1992, the District Court sentenced Manuel D...</td>\n",
       "      <td>0</td>\n",
       "      <td>In 1992, the District Court sentenced Manuel D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2476</th>\n",
       "      <td>TRAIN_2476</td>\n",
       "      <td>Immigration and Naturalization Service</td>\n",
       "      <td>St. Cyr</td>\n",
       "      <td>On March 8, 1996, Enrico St. Cyr, a lawful per...</td>\n",
       "      <td>0</td>\n",
       "      <td>On March 8, 1996, Enrico St. Cyr, a lawful per...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2477</th>\n",
       "      <td>TRAIN_2477</td>\n",
       "      <td>Markman</td>\n",
       "      <td>Westview Instruments, Inc.</td>\n",
       "      <td>Herbert Markman owns the patent to a system th...</td>\n",
       "      <td>0</td>\n",
       "      <td>Herbert Markman owns the patent to a system th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2478 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              ID                                   first_party  \\\n",
       "0     TRAIN_0000                             Phil A. St. Amant   \n",
       "1     TRAIN_0001                                Stephen Duncan   \n",
       "2     TRAIN_0002                             Billy Joe Magwood   \n",
       "3     TRAIN_0003                                    Linkletter   \n",
       "4     TRAIN_0004                            William Earl Fikes   \n",
       "...          ...                                           ...   \n",
       "2473  TRAIN_2473  HollyFrontier Cheyenne Refining, LLC, et al.   \n",
       "2474  TRAIN_2474           Grupo Mexicano de Desarrollo, S. A.   \n",
       "2475  TRAIN_2475                                       Peguero   \n",
       "2476  TRAIN_2476        Immigration and Naturalization Service   \n",
       "2477  TRAIN_2477                                       Markman   \n",
       "\n",
       "                             second_party  \\\n",
       "0                      Herman A. Thompson   \n",
       "1                          Lawrence Owens   \n",
       "2          Tony Patterson, Warden, et al.   \n",
       "3                                  Walker   \n",
       "4                                 Alabama   \n",
       "...                                   ...   \n",
       "2473  Renewable Fuels Association, et al.   \n",
       "2474             Alliance Bond Fund, Inc.   \n",
       "2475                        United States   \n",
       "2476                              St. Cyr   \n",
       "2477           Westview Instruments, Inc.   \n",
       "\n",
       "                                                  facts  first_party_winner  \\\n",
       "0     On June 27, 1962, Phil St. Amant, a candidate ...                   1   \n",
       "1     Ramon Nelson was riding his bike when he suffe...                   0   \n",
       "2     An Alabama state court convicted Billy Joe Mag...                   1   \n",
       "3     Victor Linkletter was convicted in state court...                   0   \n",
       "4     On April 24, 1953 in Selma, Alabama, an intrud...                   1   \n",
       "...                                                 ...                 ...   \n",
       "2473  Congress amended the Clean Air Act through the...                   1   \n",
       "2474  Alliance Bond Fund, Inc., an investment fund, ...                   1   \n",
       "2475  In 1992, the District Court sentenced Manuel D...                   0   \n",
       "2476  On March 8, 1996, Enrico St. Cyr, a lawful per...                   0   \n",
       "2477  Herbert Markman owns the patent to a system th...                   0   \n",
       "\n",
       "                                          facts_replace  \n",
       "0     On June 27, 1962, Phil St. Amant, a candidate ...  \n",
       "1     Ramon Nelson was riding his bike when he suffe...  \n",
       "2     An Alabama state court convicted Billy Joe Mag...  \n",
       "3     Victor Linkletter was convicted in state court...  \n",
       "4     On April 24, 1953 in Selma, Alabama, an intrud...  \n",
       "...                                                 ...  \n",
       "2473  Congress amended the Clean Air Act through the...  \n",
       "2474  Alliance Bond Fund, Inc., an investment fund, ...  \n",
       "2475  In 1992, the District Court sentenced Manuel D...  \n",
       "2476  On March 8, 1996, Enrico St. Cyr, a lawful per...  \n",
       "2477  Herbert Markman owns the patent to a system th...  \n",
       "\n",
       "[2478 rows x 6 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 특수문자 정리\n",
    "df['facts_replace'] = df['facts'].str.replace(r'<[^<>]*>', '', regex=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf72577a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['On',\n",
       " 'June',\n",
       " '27',\n",
       " ',',\n",
       " '1962',\n",
       " ',',\n",
       " 'Phil',\n",
       " 'St.',\n",
       " 'Amant',\n",
       " ',',\n",
       " 'a',\n",
       " 'candidate',\n",
       " 'for',\n",
       " 'public',\n",
       " 'office',\n",
       " ',',\n",
       " 'made',\n",
       " 'a',\n",
       " 'television',\n",
       " 'speech',\n",
       " 'in',\n",
       " 'Baton',\n",
       " 'Rouge',\n",
       " ',',\n",
       " 'Louisiana',\n",
       " '.',\n",
       " 'During',\n",
       " 'this',\n",
       " 'speech',\n",
       " ',',\n",
       " 'St.',\n",
       " 'Amant',\n",
       " 'accused',\n",
       " 'his',\n",
       " 'political',\n",
       " 'opponent',\n",
       " 'of',\n",
       " 'being',\n",
       " 'a',\n",
       " 'Communist',\n",
       " 'and',\n",
       " 'of',\n",
       " 'being',\n",
       " 'involved',\n",
       " 'in',\n",
       " 'criminal',\n",
       " 'activities',\n",
       " 'with',\n",
       " 'the',\n",
       " 'head',\n",
       " 'of',\n",
       " 'the',\n",
       " 'local',\n",
       " 'Teamsters',\n",
       " 'Union',\n",
       " '.',\n",
       " 'Finally',\n",
       " ',',\n",
       " 'St.',\n",
       " 'Amant',\n",
       " 'implicated',\n",
       " 'Herman',\n",
       " 'Thompson',\n",
       " ',',\n",
       " 'an',\n",
       " 'East',\n",
       " 'Baton',\n",
       " 'Rouge',\n",
       " 'deputy',\n",
       " 'sheriff',\n",
       " ',',\n",
       " 'in',\n",
       " 'a',\n",
       " 'scheme',\n",
       " 'to',\n",
       " 'move',\n",
       " 'money',\n",
       " 'between',\n",
       " 'the',\n",
       " 'Teamsters',\n",
       " 'Union',\n",
       " 'and',\n",
       " 'St.',\n",
       " 'Amant',\n",
       " '’',\n",
       " 's',\n",
       " 'political',\n",
       " 'opponent',\n",
       " '.',\n",
       " 'Thompson',\n",
       " 'successfully',\n",
       " 'sued',\n",
       " 'St.',\n",
       " 'Amant',\n",
       " 'for',\n",
       " 'defamation',\n",
       " '.',\n",
       " 'Louisiana',\n",
       " '’',\n",
       " 's',\n",
       " 'First',\n",
       " 'Circuit',\n",
       " 'Court',\n",
       " 'of',\n",
       " 'Appeals',\n",
       " 'reversed',\n",
       " ',',\n",
       " 'holding',\n",
       " 'that',\n",
       " 'Thompson',\n",
       " 'did',\n",
       " 'not',\n",
       " 'show',\n",
       " 'St.',\n",
       " 'Amant',\n",
       " 'acted',\n",
       " 'with',\n",
       " '“',\n",
       " 'malice.',\n",
       " '”',\n",
       " 'Thompson',\n",
       " 'then',\n",
       " 'appealed',\n",
       " 'to',\n",
       " 'the',\n",
       " 'Supreme',\n",
       " 'Court',\n",
       " 'of',\n",
       " 'Louisiana',\n",
       " '.',\n",
       " 'That',\n",
       " 'court',\n",
       " 'held',\n",
       " 'that',\n",
       " ',',\n",
       " 'although',\n",
       " 'public',\n",
       " 'figures',\n",
       " 'forfeit',\n",
       " 'some',\n",
       " 'of',\n",
       " 'their',\n",
       " 'First',\n",
       " 'Amendment',\n",
       " 'protection',\n",
       " 'from',\n",
       " 'defamation',\n",
       " ',',\n",
       " 'St.',\n",
       " 'Amant',\n",
       " 'accused',\n",
       " 'Thompson',\n",
       " 'of',\n",
       " 'a',\n",
       " 'crime',\n",
       " 'with',\n",
       " 'utter',\n",
       " 'disregard',\n",
       " 'of',\n",
       " 'whether',\n",
       " 'the',\n",
       " 'remarks',\n",
       " 'were',\n",
       " 'true',\n",
       " '.',\n",
       " 'Finally',\n",
       " ',',\n",
       " 'that',\n",
       " 'court',\n",
       " 'held',\n",
       " 'that',\n",
       " 'the',\n",
       " 'First',\n",
       " 'Amendment',\n",
       " 'protects',\n",
       " 'uninhibited',\n",
       " ',',\n",
       " 'robust',\n",
       " 'debate',\n",
       " ',',\n",
       " 'rather',\n",
       " 'than',\n",
       " 'an',\n",
       " 'open',\n",
       " 'season',\n",
       " 'to',\n",
       " 'shoot',\n",
       " 'down',\n",
       " 'the',\n",
       " 'good',\n",
       " 'name',\n",
       " 'of',\n",
       " 'anyone',\n",
       " 'who',\n",
       " 'happens',\n",
       " 'to',\n",
       " 'be',\n",
       " 'a',\n",
       " 'public',\n",
       " 'servant',\n",
       " '.',\n",
       " 'Ramon',\n",
       " 'Nelson',\n",
       " 'was',\n",
       " 'riding',\n",
       " 'his',\n",
       " 'bike',\n",
       " 'when',\n",
       " 'he',\n",
       " 'suffered',\n",
       " 'a',\n",
       " 'lethal',\n",
       " 'blow',\n",
       " 'to',\n",
       " 'the',\n",
       " 'back',\n",
       " 'of',\n",
       " 'his',\n",
       " 'head',\n",
       " 'with',\n",
       " 'a',\n",
       " 'baseball',\n",
       " 'bat',\n",
       " '.',\n",
       " 'After',\n",
       " 'two',\n",
       " 'eyewitnesses',\n",
       " 'identified',\n",
       " 'Lawrence',\n",
       " 'Owens',\n",
       " 'from',\n",
       " 'an',\n",
       " 'array',\n",
       " 'of',\n",
       " 'photos',\n",
       " 'and',\n",
       " 'then',\n",
       " 'a',\n",
       " 'lineup',\n",
       " ',',\n",
       " 'he',\n",
       " 'was',\n",
       " 'tried',\n",
       " 'and',\n",
       " 'convicted',\n",
       " 'for',\n",
       " 'Nelson',\n",
       " '’',\n",
       " 's',\n",
       " 'death',\n",
       " '.',\n",
       " 'Because',\n",
       " 'Nelson',\n",
       " 'was',\n",
       " 'carrying',\n",
       " 'cocaine',\n",
       " 'and',\n",
       " 'crack',\n",
       " 'cocaine',\n",
       " 'potentially',\n",
       " 'for',\n",
       " 'distribution',\n",
       " ',',\n",
       " 'the',\n",
       " 'judge',\n",
       " 'at',\n",
       " 'Owens',\n",
       " '’',\n",
       " 'bench',\n",
       " 'trial',\n",
       " 'ruled',\n",
       " 'that',\n",
       " 'Owens',\n",
       " 'was',\n",
       " 'probably',\n",
       " 'also',\n",
       " 'a',\n",
       " 'drug',\n",
       " 'dealer',\n",
       " 'and',\n",
       " 'was',\n",
       " 'trying',\n",
       " 'to',\n",
       " '“',\n",
       " 'knock',\n",
       " '[',\n",
       " 'Nelson',\n",
       " ']',\n",
       " 'off.',\n",
       " '”',\n",
       " 'Owens',\n",
       " 'was',\n",
       " 'found',\n",
       " 'guilty',\n",
       " 'of',\n",
       " 'first-degree',\n",
       " 'murder',\n",
       " 'and',\n",
       " 'sentenced',\n",
       " 'to',\n",
       " '25',\n",
       " 'years',\n",
       " 'in',\n",
       " 'prison',\n",
       " '.',\n",
       " 'Owens',\n",
       " 'filed',\n",
       " 'a',\n",
       " 'petition',\n",
       " 'for',\n",
       " 'a',\n",
       " 'writ',\n",
       " 'of',\n",
       " 'habeas',\n",
       " 'corpus',\n",
       " 'on',\n",
       " 'the',\n",
       " 'grounds',\n",
       " 'that',\n",
       " 'his',\n",
       " 'constitutional',\n",
       " 'right',\n",
       " 'to',\n",
       " 'due',\n",
       " 'process',\n",
       " 'was',\n",
       " 'violated',\n",
       " 'during',\n",
       " 'the',\n",
       " 'trial',\n",
       " '.',\n",
       " 'He',\n",
       " 'argued',\n",
       " 'that',\n",
       " 'the',\n",
       " 'eyewitness',\n",
       " 'identification',\n",
       " 'should',\n",
       " 'have',\n",
       " 'been',\n",
       " 'inadmissible',\n",
       " 'based',\n",
       " 'on',\n",
       " 'unreliability',\n",
       " 'and',\n",
       " 'that',\n",
       " 'the',\n",
       " 'judge',\n",
       " 'impermissibly',\n",
       " 'inferred',\n",
       " 'a',\n",
       " 'motive',\n",
       " 'when',\n",
       " 'a',\n",
       " 'motive',\n",
       " 'was',\n",
       " 'not',\n",
       " 'an',\n",
       " 'element',\n",
       " 'of',\n",
       " 'the',\n",
       " 'offense',\n",
       " '.',\n",
       " 'The',\n",
       " 'district',\n",
       " 'court',\n",
       " 'denied',\n",
       " 'the',\n",
       " 'writ',\n",
       " 'of',\n",
       " 'habeas',\n",
       " 'corpus',\n",
       " ',',\n",
       " 'and',\n",
       " 'Owens',\n",
       " 'appealed',\n",
       " '.',\n",
       " 'The',\n",
       " 'U.S.',\n",
       " 'Court',\n",
       " 'of',\n",
       " 'Appeals',\n",
       " 'for',\n",
       " 'the',\n",
       " 'Seventh',\n",
       " 'Circuit',\n",
       " 'reversed',\n",
       " 'the',\n",
       " 'denial',\n",
       " 'and',\n",
       " 'held',\n",
       " 'that',\n",
       " 'the',\n",
       " 'trial',\n",
       " 'judge',\n",
       " '’',\n",
       " 's',\n",
       " 'inference',\n",
       " 'about',\n",
       " 'Owens',\n",
       " '’',\n",
       " 's',\n",
       " 'motive',\n",
       " 'violated',\n",
       " 'his',\n",
       " 'right',\n",
       " 'to',\n",
       " 'have',\n",
       " 'his',\n",
       " 'guilt',\n",
       " 'adjudicated',\n",
       " 'solely',\n",
       " 'based',\n",
       " 'on',\n",
       " 'the',\n",
       " 'evidence',\n",
       " 'presented',\n",
       " 'at',\n",
       " 'trial',\n",
       " '.',\n",
       " 'An',\n",
       " 'Alabama',\n",
       " 'state',\n",
       " 'court',\n",
       " 'convicted',\n",
       " 'Billy',\n",
       " 'Joe',\n",
       " 'Magwood',\n",
       " 'of',\n",
       " 'murder',\n",
       " 'and',\n",
       " 'sentenced',\n",
       " 'him',\n",
       " 'to',\n",
       " 'death',\n",
       " '.',\n",
       " 'Subsequently',\n",
       " ',',\n",
       " 'an',\n",
       " 'Alabama',\n",
       " 'federal',\n",
       " 'district',\n",
       " 'court',\n",
       " 'partially',\n",
       " 'granted',\n",
       " 'Mr.',\n",
       " 'Magwood',\n",
       " \"'s\",\n",
       " 'petition',\n",
       " 'for',\n",
       " 'federal',\n",
       " 'habeas',\n",
       " 'corpus',\n",
       " 'relief',\n",
       " '.',\n",
       " 'The',\n",
       " 'court',\n",
       " 'upheld',\n",
       " 'his',\n",
       " 'conviction',\n",
       " 'but',\n",
       " 'instructed',\n",
       " 'the',\n",
       " 'state',\n",
       " 'court',\n",
       " 'to',\n",
       " 'look',\n",
       " 'at',\n",
       " 'mitigating',\n",
       " 'evidence',\n",
       " 'when',\n",
       " 'resentencing',\n",
       " 'Mr.',\n",
       " 'Magwood',\n",
       " '.',\n",
       " 'Upon',\n",
       " 'resentencing',\n",
       " ',',\n",
       " 'the',\n",
       " 'state',\n",
       " 'court',\n",
       " 'sentenced',\n",
       " 'Mr.',\n",
       " 'Magwood',\n",
       " 'to',\n",
       " 'death',\n",
       " 'once',\n",
       " 'again',\n",
       " '.',\n",
       " 'Mr.',\n",
       " 'Magwood',\n",
       " 'filed',\n",
       " 'a',\n",
       " 'second',\n",
       " 'petition',\n",
       " 'for',\n",
       " 'federal',\n",
       " 'habeas',\n",
       " 'corpus',\n",
       " 'relief',\n",
       " 'with',\n",
       " 'the',\n",
       " 'federal',\n",
       " 'district',\n",
       " 'court',\n",
       " 'arguing',\n",
       " 'that',\n",
       " 'a',\n",
       " 'judicial',\n",
       " 'rule',\n",
       " 'was',\n",
       " 'retroactively',\n",
       " 'applied',\n",
       " 'in',\n",
       " 'his',\n",
       " 'case',\n",
       " 'and',\n",
       " 'that',\n",
       " 'he',\n",
       " 'lacked',\n",
       " 'effective',\n",
       " 'counsel',\n",
       " 'at',\n",
       " 'sentencing',\n",
       " '.',\n",
       " 'The',\n",
       " 'district',\n",
       " 'court',\n",
       " 'granted',\n",
       " 'the',\n",
       " 'petition',\n",
       " 'and',\n",
       " 'vacated',\n",
       " 'Mr.',\n",
       " 'Magwood',\n",
       " \"'s\",\n",
       " 'death',\n",
       " 'sentence',\n",
       " '.',\n",
       " 'On',\n",
       " 'appeal',\n",
       " ',',\n",
       " 'the',\n",
       " 'U.S.',\n",
       " 'Court',\n",
       " 'of',\n",
       " 'Appeals',\n",
       " 'for',\n",
       " 'the',\n",
       " 'Eleventh',\n",
       " 'circuit',\n",
       " 'reversed',\n",
       " ',',\n",
       " 'holding',\n",
       " 'that',\n",
       " 'prisoners',\n",
       " 'may',\n",
       " 'not',\n",
       " 'raise',\n",
       " 'challenges',\n",
       " 'to',\n",
       " 'an',\n",
       " 'original',\n",
       " 'sentence',\n",
       " 'that',\n",
       " 'could',\n",
       " 'have',\n",
       " 'been',\n",
       " 'raised',\n",
       " 'in',\n",
       " 'an',\n",
       " 'earlier',\n",
       " 'petition',\n",
       " '.',\n",
       " 'The',\n",
       " 'court',\n",
       " 'also',\n",
       " 'held',\n",
       " 'that',\n",
       " 'Mr.',\n",
       " 'Magwood',\n",
       " \"'s\",\n",
       " 'counsel',\n",
       " 'was',\n",
       " 'not',\n",
       " 'ineffective',\n",
       " 'because',\n",
       " 'he',\n",
       " 'failed',\n",
       " 'to',\n",
       " 'raise',\n",
       " 'an',\n",
       " 'argument',\n",
       " 'that',\n",
       " 'had',\n",
       " 'already',\n",
       " 'been',\n",
       " 'decided',\n",
       " 'by',\n",
       " 'the',\n",
       " 'state',\n",
       " \"'s\",\n",
       " 'highest',\n",
       " 'court',\n",
       " 'adverse',\n",
       " 'to',\n",
       " 'his',\n",
       " 'client',\n",
       " \"'s\",\n",
       " 'position',\n",
       " '.',\n",
       " 'Victor',\n",
       " 'Linkletter',\n",
       " 'was',\n",
       " 'convicted',\n",
       " 'in',\n",
       " 'state',\n",
       " 'court',\n",
       " 'on',\n",
       " 'evidence',\n",
       " 'illegally',\n",
       " 'obtained',\n",
       " 'by',\n",
       " 'police',\n",
       " 'prior',\n",
       " 'to',\n",
       " 'the',\n",
       " 'Supreme',\n",
       " 'Court',\n",
       " 'decision',\n",
       " 'concerning',\n",
       " 'the',\n",
       " 'Fourth',\n",
       " 'Amendment',\n",
       " 'in',\n",
       " 'Mapp',\n",
       " 'v.',\n",
       " 'Ohio',\n",
       " '.',\n",
       " 'Mapp',\n",
       " 'applied',\n",
       " 'the',\n",
       " 'exclusionary',\n",
       " 'rule',\n",
       " 'to',\n",
       " 'state',\n",
       " 'criminal',\n",
       " 'proceedings',\n",
       " ',',\n",
       " 'denying',\n",
       " 'the',\n",
       " 'use',\n",
       " 'of',\n",
       " 'illegally',\n",
       " 'obtained',\n",
       " 'evidence',\n",
       " 'at',\n",
       " 'trial',\n",
       " '.',\n",
       " 'Linkletter',\n",
       " 'argued',\n",
       " 'for',\n",
       " 'a',\n",
       " 'retrial',\n",
       " 'based',\n",
       " 'on',\n",
       " 'the',\n",
       " 'Mapp',\n",
       " 'decision',\n",
       " '.',\n",
       " 'On',\n",
       " 'April',\n",
       " '24',\n",
       " ',',\n",
       " '1953',\n",
       " 'in',\n",
       " 'Selma',\n",
       " ',',\n",
       " 'Alabama',\n",
       " ',',\n",
       " 'an',\n",
       " 'intruder',\n",
       " 'broke',\n",
       " 'into',\n",
       " 'the',\n",
       " 'apartment',\n",
       " 'of',\n",
       " 'the',\n",
       " 'daughter',\n",
       " 'of',\n",
       " 'the',\n",
       " 'city',\n",
       " 'mayor',\n",
       " '.',\n",
       " 'The',\n",
       " 'daughter',\n",
       " 'and',\n",
       " 'the',\n",
       " 'intruder',\n",
       " 'struggled',\n",
       " 'through',\n",
       " 'several',\n",
       " 'rooms',\n",
       " 'until',\n",
       " 'she',\n",
       " 'was',\n",
       " 'able',\n",
       " 'to',\n",
       " 'seize',\n",
       " 'his',\n",
       " 'knife',\n",
       " ',',\n",
       " 'and',\n",
       " 'he',\n",
       " 'fled',\n",
       " '.',\n",
       " 'The',\n",
       " 'assailant',\n",
       " 'had',\n",
       " 'a',\n",
       " 'towel',\n",
       " 'over',\n",
       " 'his',\n",
       " 'head',\n",
       " ',',\n",
       " 'so',\n",
       " 'the',\n",
       " 'victim',\n",
       " 'could',\n",
       " 'not',\n",
       " 'identify',\n",
       " 'the',\n",
       " 'defendant',\n",
       " 'during',\n",
       " 'the',\n",
       " 'trial',\n",
       " '.',\n",
       " 'The',\n",
       " 'police',\n",
       " 'apprehended',\n",
       " 'William',\n",
       " 'Earl',\n",
       " 'Fikes',\n",
       " 'on',\n",
       " 'the',\n",
       " 'basis',\n",
       " 'of',\n",
       " 'a',\n",
       " 'call',\n",
       " 'from',\n",
       " 'a',\n",
       " 'private',\n",
       " 'citizen',\n",
       " 'and',\n",
       " 'held',\n",
       " 'him',\n",
       " '“',\n",
       " 'on',\n",
       " 'an',\n",
       " 'open',\n",
       " 'charge',\n",
       " 'of',\n",
       " 'investigation.',\n",
       " '”',\n",
       " 'The',\n",
       " 'police',\n",
       " 'questioned',\n",
       " 'Fikes',\n",
       " 'for',\n",
       " 'hours',\n",
       " ',',\n",
       " 'placed',\n",
       " 'him',\n",
       " 'in',\n",
       " 'jail',\n",
       " ',',\n",
       " 'and',\n",
       " 'limited',\n",
       " 'his',\n",
       " 'access',\n",
       " 'to',\n",
       " 'anyone',\n",
       " 'familiar',\n",
       " '.',\n",
       " 'After',\n",
       " 'nearly',\n",
       " 'a',\n",
       " 'week',\n",
       " 'of',\n",
       " 'this',\n",
       " 'treatment',\n",
       " ',',\n",
       " 'Fikes',\n",
       " 'confessed',\n",
       " 'in',\n",
       " 'the',\n",
       " 'form',\n",
       " 'of',\n",
       " 'answers',\n",
       " 'to',\n",
       " 'the',\n",
       " 'interrogator',\n",
       " '’',\n",
       " 's',\n",
       " 'leading',\n",
       " 'questions',\n",
       " '.',\n",
       " 'Five',\n",
       " 'days',\n",
       " 'later',\n",
       " ',',\n",
       " 'Fikes',\n",
       " 'confessed',\n",
       " 'under',\n",
       " 'questioning',\n",
       " 'a',\n",
       " 'second',\n",
       " 'time',\n",
       " '.',\n",
       " 'When',\n",
       " 'these',\n",
       " 'confessions',\n",
       " 'were',\n",
       " 'admitted',\n",
       " 'into',\n",
       " 'the',\n",
       " 'trial',\n",
       " 'as',\n",
       " 'evidence',\n",
       " ',',\n",
       " 'Fikes',\n",
       " 'did',\n",
       " 'not',\n",
       " 'testify',\n",
       " 'regarding',\n",
       " 'the',\n",
       " 'events',\n",
       " 'surrounding',\n",
       " 'his',\n",
       " 'interrogation',\n",
       " 'because',\n",
       " 'the',\n",
       " 'judge',\n",
       " 'had',\n",
       " 'ruled',\n",
       " 'he',\n",
       " 'would',\n",
       " 'be',\n",
       " 'subjected',\n",
       " 'to',\n",
       " 'unlimited',\n",
       " 'cross-examination',\n",
       " '.',\n",
       " 'The',\n",
       " 'jury',\n",
       " 'convicted',\n",
       " 'Fikes',\n",
       " 'and',\n",
       " 'sentenced',\n",
       " 'him',\n",
       " 'to',\n",
       " 'death',\n",
       " '.',\n",
       " 'The',\n",
       " 'Supreme',\n",
       " 'Court',\n",
       " 'of',\n",
       " 'Alabama',\n",
       " 'affirmed',\n",
       " '.',\n",
       " 'A',\n",
       " 'New',\n",
       " 'York',\n",
       " 'town',\n",
       " ',',\n",
       " 'Clarkstown',\n",
       " ',',\n",
       " 'allowed',\n",
       " 'a',\n",
       " 'contractor',\n",
       " 'to',\n",
       " 'construct',\n",
       " 'and',\n",
       " 'operate',\n",
       " 'a',\n",
       " 'waste',\n",
       " 'processing',\n",
       " 'plant',\n",
       " 'within',\n",
       " 'town',\n",
       " 'limits',\n",
       " '.',\n",
       " 'The',\n",
       " 'revenue',\n",
       " 'from',\n",
       " 'the',\n",
       " 'plant',\n",
       " 'would',\n",
       " 'help',\n",
       " 'compensate',\n",
       " 'the',\n",
       " 'contractor',\n",
       " '.',\n",
       " 'Clarkstown',\n",
       " 'promised',\n",
       " 'that',\n",
       " 'the',\n",
       " 'plant',\n",
       " 'would',\n",
       " 'receive',\n",
       " '120,000',\n",
       " 'tons',\n",
       " 'of',\n",
       " 'solid',\n",
       " 'waste',\n",
       " 'each',\n",
       " 'year',\n",
       " ',',\n",
       " 'and',\n",
       " 'permitted',\n",
       " 'the',\n",
       " 'contractor',\n",
       " 'to',\n",
       " 'charge',\n",
       " 'an',\n",
       " '$',\n",
       " '81',\n",
       " '``',\n",
       " 'tipping',\n",
       " 'fee',\n",
       " \"''\",\n",
       " 'for',\n",
       " 'each',\n",
       " 'ton',\n",
       " 'received',\n",
       " '.',\n",
       " 'To',\n",
       " 'meet',\n",
       " 'the',\n",
       " '120,000',\n",
       " 'ton',\n",
       " 'quota',\n",
       " ',',\n",
       " 'Clarkstown',\n",
       " 'adopted',\n",
       " 'a',\n",
       " '``',\n",
       " 'flow',\n",
       " 'control',\n",
       " 'ordinance',\n",
       " '.',\n",
       " \"''\",\n",
       " 'The',\n",
       " 'ordinance',\n",
       " 'required',\n",
       " 'that',\n",
       " 'all',\n",
       " 'solid',\n",
       " 'waste',\n",
       " 'flowing',\n",
       " 'into',\n",
       " 'and',\n",
       " 'out',\n",
       " 'of',\n",
       " 'the',\n",
       " 'town',\n",
       " 'pass',\n",
       " 'through',\n",
       " 'the',\n",
       " 'new',\n",
       " 'plant',\n",
       " '.',\n",
       " 'C',\n",
       " '&',\n",
       " 'A',\n",
       " 'Carbone',\n",
       " ',',\n",
       " 'Inc.',\n",
       " 'operated',\n",
       " 'a',\n",
       " 'similar',\n",
       " 'plant',\n",
       " 'within',\n",
       " 'the',\n",
       " 'town',\n",
       " '.',\n",
       " 'To',\n",
       " 'avoid',\n",
       " 'paying',\n",
       " 'the',\n",
       " '$',\n",
       " '81',\n",
       " 'fee',\n",
       " ',',\n",
       " 'Carbone',\n",
       " 'trucked',\n",
       " 'processed',\n",
       " 'waste',\n",
       " 'directly',\n",
       " 'to',\n",
       " ...]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 전체 facts 를 하나로\n",
    "corpus = df['facts_replace'].str.cat(sep=\" \")\n",
    "\n",
    "# 전체에 대한 토큰화\n",
    "lst_tokens = nltk.tokenize.word_tokenize(corpus)\n",
    "lst_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c43c97fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/philinggood/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 불용어 (I, my, me, to와 같이 분석에 의미없는 빈번한 단어) 제거\n",
    "# nltk에서 제공해주는 불용어 정의 가져오기\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "35453501",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 불용어 리스트 저장\n",
    "lst_stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "lst_stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d2dd02",
   "metadata": {},
   "source": [
    "### Stemmer\n",
    "* 영어에서 동사의 굴절 어미인, -s, -ing, -(e)d 등을 제거하여 say(sai)와 같은 어간(stem)만을 추출한다. 예) eating = eat\n",
    "* 가끔 쌩뚱맞은 단어로 변형할 수 있다. 예) caring -> car, care가 car가 돼버린다. 영어는 복잡하다.\n",
    "* 토큰화 여부 상관 없음\n",
    "* 출처 : https://www.projectpro.io/recipes/use-porter-stemmer\n",
    "```python\n",
    "ps = nltk.stem.porter.PorterStemmer()\n",
    "```\n",
    "\n",
    "### Lemmatizer\n",
    "* 영단어에 굴절이 일어나기 이전의 원형을 'lemma'라고 하며 lemma로 복원하는 작업이다. 예) caring -> care, said -> say\n",
    "* PorterStemmer 보다는 조금 더 유의미하게 변형시켜준다. 하지만 단어형태를 바꾸진 않는다. 예) are -> are\n",
    "* 토큰화 하고 해야함\n",
    "* 출처 : https://www.machinelearningplus.com/nlp/lemmatization-examples-python/\n",
    "```python\n",
    "lem = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "```\n",
    "\n",
    "### spacy\n",
    "* spacy는 위의 방법들 보다는 더 단어의 원형태로 변형시켜준다. 예) are -> be\n",
    "* 단 pronouns(대명사)는 -PRON-으로 나타냄을 볼 수 있다.\n",
    "* 토큰화 하고 해야함\n",
    "* 출처 : https://www.machinelearningplus.com/nlp/lemmatization-examples-python/\n",
    "```python\n",
    "scy = spacy.load('en_core_web_sm', disable=['parser','ner'])\n",
    "\n",
    "# 예제\n",
    "sentence = \"The striped bats are hanging on their feet for best\"\n",
    "\n",
    "# Parse the sentence using the loaded 'en' model object `nlp`\n",
    "doc = nlp(sentence)\n",
    "\n",
    "# Extract the lemma for each token and join\n",
    "\" \".join([token.lemma_ for token in doc])\n",
    "#> 'the strip bat be hang on -PRON- foot for good'\n",
    "```\n",
    "\n",
    "## spacy -> stemming & lemmatisation 하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf480da6",
   "metadata": {},
   "source": [
    "### spacy 적용 코드 여러가지 작성하다가 가장 간단한 한 줄(방법4) 채택\n",
    "```python\n",
    "def spacy_preprocessing(sentence):\n",
    "    # Parse the sentence using the loaded 'en' model object `nlp`\n",
    "    doc = nlp(sentence)\n",
    "\n",
    "    # Extract the lemma for each token and join\n",
    "    text = \" \".join([token.lemma_ for token in doc])\n",
    "    #> 'the strip bat be hang on -PRON- foot for good'\n",
    "    return text\n",
    "\n",
    "# 방법 1\n",
    "# 함수에 str로 바로 넣기, list 저장 후 df로 추가\n",
    "new_sentence = []\n",
    "for sentence in tqdm(df['facts_clean']):\n",
    "    new_sentence.append(spacy_preprocessing(sentence))\n",
    "df['facts_spacy'] = new_sentence\n",
    "\n",
    "# 방법 2\n",
    "# index 로 받아서 df에 하나씩 변환하여 추가\n",
    "for i in range(len(df)):\n",
    "    df.loc[i, 'facts_spacy'] = spacy_preprocessing(df.loc[i, 'facts_clean'])\n",
    "\n",
    "# 방법 3\n",
    "# 하나씩 \"함수\" 적용해서 추가\n",
    "df['facts_new'] = df['facts_clean'].apply(lambda x: spacy_preprocessing(x))\n",
    "\n",
    "# 방법 4\n",
    "# 하나씩 적용해서 추가 (함수 없이)\n",
    "df['facts_done'] = df['facts_clean'].apply(lambda x: \" \".join([token.lemma_ for token in nlp(x)]))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2e40bc6f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/philinggood/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d8c66993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 정의\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])\n",
    "\n",
    "df['facts_spacy'] = df['facts_replace'].apply(lambda x: \" \".join([token.lemma_ for token in nlp(x)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "32d51b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰화, stemming, lemmatisation\n",
    "def utils_preprocess_text(text, flg_stemm=False, flg_lemm=True, lst_stopwords=None):\n",
    "    # 전처리: 특수문자 삭제, string, 소문자화, 공백 제거 \n",
    "    text = re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n",
    "\n",
    "    # 토큰화 str -> list\n",
    "    lst_text = text.split()\n",
    "    \n",
    "    # 불용어 제거\n",
    "    if lst_stopwords is not None: \n",
    "        lst_text = [word for word in lst_text if word not in \n",
    "                    lst_stopwords]\n",
    "\n",
    "    ## Stemming (remove -ing, -ly, -ed, ...)\n",
    "    if flg_stemm == True:\n",
    "        ps = nltk.stem.porter.PorterStemmer()\n",
    "        lst_text = [ps.stem(word) for word in lst_text]\n",
    "\n",
    "    ## Lemmatisation (convert the word into root word)\n",
    "    if flg_lemm == True:\n",
    "        lem = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "        lst_text = [lem.lemmatize(word) for word in lst_text]\n",
    "\n",
    "    ## list -> str\n",
    "    text = \" \".join(lst_text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "807c5761",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       june 27 1962 phil st amant candidate public of...\n",
       "1       ramon nelson ride bike suffer lethal blow back...\n",
       "2       alabama state court convict billy joe magwood ...\n",
       "3       victor linkletter convict state court evidence...\n",
       "4       april 24 1953 selma alabama intruder break apa...\n",
       "                              ...                        \n",
       "2473    congress amend clean air act energy policy act...\n",
       "2474    alliance bond fund inc investment fund purchas...\n",
       "2475    1992 district court sentence manuel peguero 27...\n",
       "2476    march 8 1996 enrico st cyr lawful permanent re...\n",
       "2477    herbert markman patent system track clothing d...\n",
       "Name: facts_clean, Length: 2478, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['facts_clean'] = \\\n",
    "df['facts_spacy'].apply(lambda x: \\\n",
    "                        utils_preprocess_text(x, flg_stemm=False, \\\n",
    "                                              flg_lemm=True, lst_stopwords=lst_stopwords))\n",
    "df['facts_clean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2dea35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f7fed7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['facts'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8b4f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[0, 'facts_replace']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250dd684",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.loc[0, 'facts_spacy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f957bd7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.loc[0,'facts_clean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43efbab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시각화\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b95e3c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c70bae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 많이 사용된 단어 출력\n",
    "def plot_word(label):\n",
    "    corpus = df[df[\"first_party_winner\"]== label][\"facts_clean\"]\n",
    "    lst_tokens = nltk.tokenize.word_tokenize(corpus.str.cat(sep=\" \"))\n",
    "    fig, ax = plt.subplots(nrows=2, ncols=1)\n",
    "    fig.suptitle(\"Most frequent words\", fontsize=15)\n",
    "    figure(figsize=(30, 24))\n",
    "\n",
    "    # unigrams : 한 단어\n",
    "    dic_words_freq = nltk.FreqDist(lst_tokens)\n",
    "    dtf_uni = pd.DataFrame(dic_words_freq.most_common(), \n",
    "                           columns=[\"Word\",\"Freq\"])\n",
    "    dtf_uni.set_index(\"Word\").iloc[:10,:].sort_values(by=\"Freq\").plot(\n",
    "                      kind=\"barh\", title=\"Unigrams\", ax=ax[0], \n",
    "                      legend=False).grid(axis='x')\n",
    "    ax[0].set(ylabel=None)\n",
    "\n",
    "    # bigrams : 두 단어\n",
    "    dic_words_freq = nltk.FreqDist(nltk.ngrams(lst_tokens, 2))\n",
    "    dtf_bi = pd.DataFrame(dic_words_freq.most_common(), \n",
    "                          columns=[\"Word\",\"Freq\"])\n",
    "    dtf_bi[\"Word\"] = dtf_bi[\"Word\"].apply(lambda x: \" \".join(\n",
    "                       string for string in x) )\n",
    "    dtf_bi.set_index(\"Word\").iloc[:10,:].sort_values(by=\"Freq\").plot(\n",
    "                      kind=\"barh\", title=\"Bigrams\", ax=ax[1],\n",
    "                      legend=False).grid(axis='x')\n",
    "    ax[1].set(ylabel=None)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b866c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first_party 가 1 승소\n",
    "plot_word(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f12098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first_party 가 0 패소\n",
    "plot_word(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7ab199",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f9cdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wordcloud\n",
    "wc = wordcloud.WordCloud(background_color='black', max_words=100, \n",
    "                         max_font_size=35)\n",
    "wc = wc.generate(str(corpus))\n",
    "fig = plt.figure(num=1)\n",
    "plt.axis('off')\n",
    "plt.imshow(wc, cmap=None)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d309b6d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa86d3d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86c339c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as gensim_api\n",
    "import gensim\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe81897c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_MainTopic(label):\n",
    "    corpus = df[df[\"first_party_winner\"]==label][\"facts_clean\"]\n",
    "    ## pre-process corpus\n",
    "    lst_corpus = []\n",
    "    for string in corpus:\n",
    "        lst_words = string.split()\n",
    "        lst_grams = [\" \".join(lst_words[i:i + 2]) for i in range(0, \n",
    "                         len(lst_words), 2)]\n",
    "        lst_corpus.append(lst_grams)## map words to an id\n",
    "    id2word = gensim.corpora.Dictionary(lst_corpus)## create dictionary word:freq\n",
    "    dic_corpus = [id2word.doc2bow(word) for word in lst_corpus] ## train LDA\n",
    "    lda_model = gensim.models.ldamodel.LdaModel(corpus=dic_corpus, id2word=id2word, num_topics=7, random_state=123, update_every=1, chunksize=100, passes=10, alpha='auto', per_word_topics=True)\n",
    "\n",
    "    ## output\n",
    "    lst_dics = []\n",
    "    for i in range(0,3):\n",
    "        lst_tuples = lda_model.get_topic_terms(i)\n",
    "        for tupla in lst_tuples:\n",
    "            lst_dics.append({\"topic\":i, \"id\":tupla[0], \n",
    "                             \"word\":id2word[tupla[0]], \n",
    "                             \"weight\":tupla[1]})\n",
    "    dtf_topics = pd.DataFrame(lst_dics, \n",
    "                             columns=['topic','id','word','weight'])\n",
    "\n",
    "    ## plot\n",
    "    fig, ax = plt.subplots()\n",
    "    sns.barplot(y=\"word\", x=\"weight\", hue=\"topic\", data=dtf_topics, dodge=False, ax=ax).set_title('Main Topics')\n",
    "    ax.set(ylabel=\"\", xlabel=\"Word Importance\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ef745c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first_party 가 1 승소\n",
    "plot_MainTopic(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82cc8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first_party 가 0 패소\n",
    "plot_MainTopic(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0720560b",
   "metadata": {},
   "source": [
    "## 드디어 머신러닝 돌림\n",
    "* 일단 pipeline 단계 생략함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae98bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로드\n",
    "df_train = df.copy()\n",
    "df_test = pd.read_csv('open/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37248560",
   "metadata": {},
   "source": [
    "### TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9dbb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 빈도수 카운트 하여, 카운트한 내용으로 선형회귀 모델 학습 -> 1/0을 구함\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer # 단어의 한 문서내 빈도수, 전체 문서 빈도수를 계산하여 최종 단어 빈도수 계산해주는 알고리즘\n",
    "from sklearn.linear_model import LogisticRegression # 선형 회귀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613d2a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 처리\n",
    "\n",
    "# 모델링\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# 데이터 처리 함수 생성\n",
    "def get_vector(verctorizer, df, train_mode):\n",
    "    if train_mode:\n",
    "        X_facts = vectorizer.fit_transform(df['facts'])\n",
    "    else:\n",
    "        X_facts = vectorizer.transform(df['facts'])\n",
    "    X_party1 = vectorizer.transform(df['first_party'])\n",
    "    X_party2 = vectorizer.transform(df['second_party'])\n",
    "    \n",
    "    # csr_matrix -> matrix 로 변환해줌\n",
    "    # concatenate : 배열 합치기 / axis=0(위->아래), axis=1(좌->우)\n",
    "    X = np.concatenate([X_party1.todense(),\n",
    "                       X_party2.todense(),\n",
    "                       X_facts.todense(),], axis=1)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef2a695",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = get_vector(vectorizer, df_train, True)\n",
    "Y_train = df_train[\"first_party_winner\"]\n",
    "X_test = get_vector(vectorizer, df_test, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66de46e4",
   "metadata": {},
   "source": [
    "### LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f8fdc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 회귀 모델 학습\n",
    "model_lr = LogisticRegression()\n",
    "model_lr.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cd4148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측하여 제출용 csv 생성\n",
    "\n",
    "# 기존 샘플 csv 형식 위에 작성\n",
    "# submit_lr = pd.read_csv('open/sample_submission.csv')\n",
    "\n",
    "# 예측\n",
    "y_pred = model_lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c6daa8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # 예측한 결과 대입하기\n",
    "# submit_lr['first_party_winner'] = y_pred\n",
    "# # 결과 csv로 저장\n",
    "# submit_lr.to_csv('data/Tfidf_Logistic0.csv', index=False)\n",
    "# print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d984bf2",
   "metadata": {},
   "source": [
    "### RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07cbb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4951a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_frc = RandomForestClassifier()\n",
    "model_frc.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb6b403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측하여 제출용 csv 생성\n",
    "\n",
    "# 기존 샘플 csv 형식 위에 작성\n",
    "# submit_frc = pd.read_csv('open/sample_submission.csv')\n",
    "\n",
    "# 예측\n",
    "y_pred = model_frc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eba5e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 예측한 결과 대입하기\n",
    "# submit_frc['first_party_winner'] = y_pred\n",
    "# # 결과 csv로 저장\n",
    "# submit_frc.to_csv('data/Tfidf_RandomForest0.csv', index=False)\n",
    "# print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296d3f25",
   "metadata": {},
   "source": [
    "### KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2842412a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b854230",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_knn = KNeighborsClassifier()\n",
    "model_knn.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6acadb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측하여 제출용 csv 생성\n",
    "\n",
    "# 기존 샘플 csv 형식 위에 작성\n",
    "# submit_knn = pd.read_csv('open/sample_submission.csv')\n",
    "\n",
    "# 예측\n",
    "y_pred = model_knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f858073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 예측한 결과 대입하기\n",
    "# submit_knn['first_party_winner'] = y_pred\n",
    "# # 결과 csv로 저장\n",
    "# submit_knn.to_csv('data/Tfidf_KNeighbors0.csv', index=False)\n",
    "# print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83d8f9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e875a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습 좀 더 봐주기\n",
    "### 9. 핫인코딩 부분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb5223e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb57265",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2fcb69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd69a00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0400d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075364c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25de918",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0c2542",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7975888",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf1e2fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9b135b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e72397",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbae1064",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5d8e1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7242922c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2b33c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5444ab23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f38d1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef259656",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0c2c33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564c3720",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17780b58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710626ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d2ed75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3bbfd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05214c40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899c501e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3918d73d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95edd24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd23b07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce62e22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9521ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefe9e27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e55b66c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30e4e2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d2fdbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4e2b80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006a4816",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3ee934",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee620205",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_court_jedgement",
   "language": "python",
   "name": "tf_court_jedgement"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
